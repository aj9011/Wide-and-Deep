{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os, warnings, re, datetime, multiprocessing\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import _pickle as pickle\n",
    "from gensim.models import Doc2Vec\n",
    "from itertools import chain, combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import namedtuple, Counter\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "warnings.filterwarnings(action='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ItemOneHotGenerator(data, threshold = 30):\n",
    "    \"\"\"\n",
    "    data : Data format must be pandas Series with rows having list of items.\n",
    "              reset_index() required.\n",
    "              example : '6601000010,6601000024,....,6601000001'\n",
    "    threshold : To exclude super sparse case of cross columns\n",
    "    dependencies : collections.Counter\n",
    "                              sklearn.feature_extraction.text.CountVectorizer\n",
    "    \"\"\"    \n",
    "    cross_cnt = Counter()\n",
    "    for ii, jj in data.iteritems():\n",
    "        litem = sorted(set(jj.split(',')))\n",
    "        cross_cnt.update(litem)\n",
    "    target_lcross = set({x : cross_cnt[x] for x in cross_cnt if cross_cnt[x] > threshold}.keys()) \n",
    "    \n",
    "    cross_feature = []\n",
    "    for ii, jj in data.iteritems():\n",
    "        litem = sorted(set(jj.split(',')))\n",
    "        lcross = set(litem).intersection(target_lcross)\n",
    "        cross_feature.append(lcross)\n",
    "        \n",
    "    lcross_feature = map(lambda x:' '.join(x), cross_feature)\n",
    "    vect = CountVectorizer(tokenizer=str.split)\n",
    "    mcross_feature = vect.fit_transform(lcross_feature)\n",
    "    with open('./ItemOneHotGenerator.pkl','wb') as f:\n",
    "        pickle.dump(vect, f)\n",
    "    return pd.DataFrame(mcross_feature.todense(), columns=vect.get_feature_names())\n",
    "\n",
    "def CrossColumnGenerator(data, threshold = 5):\n",
    "    \"\"\"\n",
    "    data : Data format must be pandas Series with rows having list of items.\n",
    "              reset_index() required.\n",
    "              example : '6601000010,6601000024,....,6601000001'\n",
    "    threshold : To exclude super sparse case of cross columns\n",
    "    dependencies : collections.Counter\n",
    "                              sklearn.feature_extraction.text.CountVectorizer\n",
    "    \"\"\"\n",
    "    cross_cnt = Counter()\n",
    "    for ii, jj in data.iteritems():\n",
    "        litem = sorted(set(jj.split(',')))\n",
    "        cross_cnt.update([str(kk + '_' + ll) for kk, ll in combinations(litem, 2)])\n",
    "        \n",
    "    target_lcross = set({x : cross_cnt[x] for x in cross_cnt if cross_cnt[x] >= threshold}.keys()) \n",
    "    \n",
    "    cross_feature = []\n",
    "    for ii, jj in data.iteritems():\n",
    "        litem = sorted(set(jj.split(',')))\n",
    "        lcross = set([str(kk + '_' + ll) for kk, ll in combinations(litem, 2)]).intersection(target_lcross)\n",
    "        cross_feature.append(lcross)\n",
    "\n",
    "    lcross_feature = map(lambda x:' '.join(x), cross_feature)\n",
    "    vect = CountVectorizer(tokenizer=str.split)\n",
    "    mcross_feature = vect.fit_transform(lcross_feature)\n",
    "    with open('./CrossColumnGenerator.pkl','wb') as f:\n",
    "        pickle.dump(vect, f)\n",
    "    return pd.DataFrame(mcross_feature.todense(), columns=vect.get_feature_names())\n",
    "\n",
    "def myDoc2Vec(data, d2v_size, model_loc, epoch=None, trainTF = False):\n",
    "    \"\"\"\n",
    "    data : DataFrame([['DocID','Item']])\n",
    "    \"\"\"\n",
    "    data.columns = ['docID','item']\n",
    "    data1 = [(str(row['item']).split(), row['docID']) for idx, row in data.iterrows()]\n",
    "    data2 = namedtuple('TaggedDocument', 'words tags')\n",
    "    tagged_data2 = [data2(d, [c]) for d, c in data1]\n",
    "    \n",
    "    if trainTF:\n",
    "        model = Doc2Vec(\n",
    "            dm = 0,  # 0 : PV-DBOW\n",
    "            dbow_words = 0,  # 0 : train doc-vec only(faster)\n",
    "            window = 8, vector_size = d2v_size, alpha = 0.025, min_alpha = 0.025, seed = 0, sample= 1e-5, min_count=3, \n",
    "            workers=multiprocessing.cpu_count(), hs = 0, negative = 10)\n",
    "        model.build_vocab(tagged_data2)\n",
    "        print('New model training started')\n",
    "        model.train(documents  = tagged_data2, total_examples = data.shape[0], epochs = epoch)\n",
    "        model.save(model_loc)\n",
    "        print('New model training done. check--',model_loc)\n",
    "    else:\n",
    "        model=Doc2Vec.load(model_loc)\n",
    "        embedding_df = pd.DataFrame(\n",
    "            data = [model.infer_vector(doc.words) for doc in tagged_data2], \n",
    "            columns = [\"d2v\"+str(i) for i in range(d2v_size)])\n",
    "        return embedding_df\n",
    "    \n",
    "def train_eval_filesplit(filenames, train_rate = 0.8):\n",
    "    trainfiles = []\n",
    "    evalfiles = []\n",
    "    filecnt = len(filenames)\n",
    "    trainloop = int(round(filecnt/(filecnt - int(filecnt*train_rate)),0))\n",
    "    for ii in range(filecnt):\n",
    "        if ii % trainloop == 0:\n",
    "            evalfiles.append(filenames[ii])\n",
    "        else:\n",
    "            trainfiles.append(filenames[ii])\n",
    "    filedict = {'train_files':trainfiles,'eval_files':evalfiles}\n",
    "    return filedict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = datetime.datetime.now()\n",
    "# 주문_2014 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문_2014.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "# 주문_2015 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문_2015.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "# 주문_2016 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문_2016.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "# 주문_2017 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문_2017.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "# 주문_2018 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문_2018.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "# 주문상품_2014 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문상품_2014.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "# 주문상품_2015 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문상품_2015.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "# 주문상품_2016 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문상품_2016.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "# 주문상품_2017 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문상품_2017.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "# 주문상품_2018 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문상품_2018.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "# 주문상품출하_2014 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문상품출하_2014.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "# 주문상품출하_2015 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문상품출하_2015.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "# 주문상품출하_2016 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문상품출하_2016.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "# 주문상품출하_2017 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문상품출하_2017.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "# 주문상품출하_2018 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문상품출하_2018.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "# 상품마스터 = pd.read_csv('c:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가6차_181005/상품마스터.csv',encoding ='utf-8',dtype = 'str')\n",
    "# 조직마스터 = pd.read_csv('c:/users/kim85/OneDrive/Project_AgileSoda/원본/추가6차_181005/조직마스터.csv', encoding ='utf-8', dtype ='str')\n",
    "# 사업장= pd.read_csv('c:/users/kim85/OneDrive/Project_AgileSoda/원본/추가6차_181005/사업장.csv', encoding ='utf-8', dtype ='str')\n",
    "# 공사유형 = pd.read_csv('c:/users/kim85/OneDrive/Project_AgileSoda/원본/추가6차_181005/공사유형.csv', encoding ='utf-8', dtype ='str')\n",
    "\n",
    "# 주문 = pd.concat([주문_2014, 주문_2015, 주문_2016, 주문_2017, 주문_2018])\n",
    "# 주문상품 = pd.concat([주문상품_2014, 주문상품_2015, 주문상품_2016, 주문상품_2017, 주문상품_2018])\n",
    "# 주문상품출하 = pd.concat([주문상품출하_2014, 주문상품출하_2015, 주문상품출하_2016, 주문상품출하_2017, 주문상품출하_2018])\n",
    "\n",
    "# # 주문 처리\n",
    "# 주문 = 주문.reset_index()[['ORDE_IDEN_NUMB','CONS_IDEN_NAME','GROUPID','CLIENTID','BRANCHID',\n",
    "#                        'DELI_AREA_CODE','REGI_DATE_TIME','ORDE_USER_ID']].drop_duplicates(keep='first')\n",
    "# 주문상품 = 주문상품.reset_index()[['ORDE_IDEN_NUMB','ORDE_SEQU_NUMB','GOOD_IDEN_NUMB',\n",
    "#                            'ORDE_REQU_QUAN']].drop_duplicates(keep='first')\n",
    "# 주문상품 = 주문상품.groupby(['ORDE_IDEN_NUMB','GOOD_IDEN_NUMB'])['ORDE_REQU_QUAN'].agg('sum').reset_index()\n",
    "# 주문상품출하 =  주문상품출하.reset_index()[['ORDE_IDEN_NUMB', 'DELI_STAT_FLAG']].drop_duplicates(keep='first')\n",
    "# 주문상품출하 = 주문상품출하[주문상품출하.DELI_STAT_FLAG=='70'].ORDE_IDEN_NUMB.unique()\n",
    "\n",
    "# 주문상품_주문 = pd.merge(주문상품, 주문, on = 'ORDE_IDEN_NUMB', how = 'left')\n",
    "# 주문상품_주문_출하 = 주문상품_주문[주문상품_주문.ORDE_IDEN_NUMB.isin(주문상품출하)]\n",
    "# 주문전체 = 주문상품_주문_출하[주문상품_주문_출하.GROUPID != '101'].drop_duplicates(keep='first')\n",
    "\n",
    "# # 상품 처리\n",
    "# 상품마스터 = 상품마스터[['good_iden_numb','cate_id','good_name','good_spec','good_type','repre_good']].drop_duplicates(keep='first')\n",
    "# 상품마스터 = 상품마스터.rename(columns = {'good_iden_numb':'GOOD_IDEN_NUMB'})\n",
    "# # repre_good - Y : 옵션대표상품, N : 단품, P : 옵션상품 // good_type - 10 : 일반, 20 : 지정,60 : 공구, 70 : 안전,80 : 보안\n",
    "\n",
    "# 상품주문전체 = pd.merge(주문전체, 상품마스터, on = 'GOOD_IDEN_NUMB', how = 'left')\n",
    "# 상품주문전체 = 상품주문전체[상품주문전체.repre_good=='N'] # 옵션상품 제외\n",
    "\n",
    "# # 사업장 처리\n",
    "# 사업장 = pd.concat([\n",
    "#         사업장[['BRANCHID','AREATYPE','BRANCHBUSITYPE','BRANCHBUSICLAS','WORKID']].rename(columns={'BRANCHID':'BORGID'}), \n",
    "#         사업장[['BRANCHCD','AREATYPE','BRANCHBUSITYPE','BRANCHBUSICLAS','WORKID']].rename(columns={'BRANCHCD':'BORGID'})\n",
    "#     ], axis=0).drop_duplicates(keep='first')\n",
    "\n",
    "# 공사유형_사업장 = pd.merge(\n",
    "#     사업장, \n",
    "#     공사유형[['WORKID','WORKNM']].drop_duplicates(keep='first'), \n",
    "#     how = 'left', on = 'WORKID')\n",
    "\n",
    "# 조직마스터 = 조직마스터[(조직마스터.BORGTYPECD == 'BCH') & (조직마스터.SVCTYPECD == 'BUY')] # 사업장레벨 및 구매사만\n",
    "# 조직마스터 = pd.concat([\n",
    "#         조직마스터[['BORGID','BORGNM']], \n",
    "#         조직마스터[['BORGCD','BORGNM']].rename(columns={'BORGCD':'BORGID'})\n",
    "#     ], axis=0).drop_duplicates(keep='first')\n",
    "\n",
    "# 조직전체 = pd.merge(조직마스터, 공사유형_사업장, how = 'left', on = 'BORGID').rename(columns={'BORGID':'BRANCHID'})\n",
    "\n",
    "# df = pd.merge(상품주문전체,조직전체,how = 'left',on = 'BRANCHID')\n",
    "# df = df.drop([\n",
    "#         'ORDE_REQU_QUAN','WORKNM','DELI_AREA_CODE','GROUPID','CLIENTID','ORDE_USER_ID','BORGNM',\n",
    "#         'good_name','good_spec','repre_good'], axis = 1)\n",
    "# df = df.drop_duplicates(keep='first')\n",
    "# df.columns = ['OrderNum','ProductCode','Constructionnm','BpID','OrderTime','ProductCategory','ProductClass','Region',\n",
    "#               'BpType','BpClass','ConstructionType']\n",
    "# duration = datetime.datetime.now()-start\n",
    "# m, s = divmod(duration.seconds, 60);h, m = divmod(m, 60);print(\"[%02d:%02d:%02d]\" %(h, m, s))\n",
    "\n",
    "# 이상 데이터 불러오기 및 필요 데이터 필터링 부분.\n",
    "# df.to_csv('./Datasets/df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터 불러오기 및 필요 데이터 필터링 이후부터 작업\n",
    "# df = pd.read_csv('./Datasets/df.csv', dtype='str', encoding = 'utf-8')\n",
    "\n",
    "# # 181019\n",
    "# # 일기준 작업\n",
    "# df1 = df[['BpID','Region','BpType','BpClass','ConstructionType','OrderTime','OrderNum','ProductCode']].drop_duplicates(keep='first')\n",
    "# df1['OrderTime'] = pd.to_datetime(df1['OrderTime'])\n",
    "# df1['OrderTime'] = df1['OrderTime'].dt.date\n",
    "# df1 = df1.sort_values(['BpID','OrderTime']).reset_index(drop = True)\n",
    "# df2 = df1.groupby(['BpID','OrderTime']).agg({\n",
    "#         'OrderNum': lambda x: list(x), \n",
    "#         'ProductCode' : lambda x: list(x)}).reset_index()\n",
    "\n",
    "# nx =  3 # 이전 기록 갯수(X)\n",
    "# ny = 1 # 이후 기록 갯수(Y)\n",
    "# df2_1 = {\n",
    "#     'BpID':[], \n",
    "# # 추후 해당정보 필요할 수 있으니 일단 keep\n",
    "# #     'OrderTime':[], 'x_OrderNums':[], 'y_OrderNums':[], \n",
    "#     'x_ProductCodes':[], \n",
    "#     'y_ProductCodes':[]\n",
    "# }\n",
    "# print('Loop start!!')\n",
    "# start = datetime.datetime.now()\n",
    "# for ii in range(len(df2)-nx):\n",
    "#     targetdata = df2.iloc[ii:ii+nx+ny,].reset_index(drop=True)\n",
    "#     if len(targetdata['BpID'].unique())==1 and len(targetdata) == nx+ny: # 앞 nx+ny 가 모두 같은 사업장이고 사이즈가 nx+ny인 경우\n",
    "#         tmpdata1 = targetdata['OrderNum'].tolist()\n",
    "#         tmpdata2 = targetdata['ProductCode'].tolist()\n",
    "#         df2_1['BpID'].append(targetdata['BpID'][0])\n",
    "# # 추후 해당정보 필요할 수 있으니 일단 keep\n",
    "# #         df2_1['OrderTime'].append(targetdata['OrderTime'][nx-ny])\n",
    "# #         df2_1['x_OrderNums'].append([','.join(list(chain.from_iterable(tmpdata1[0:nx])))])\n",
    "# #         df2_1['y_OrderNums'].append([','.join(list(chain.from_iterable(tmpdata1[nx:nx+ny])))])\n",
    "#         df2_1['x_ProductCodes'].append(','.join(list(chain.from_iterable(tmpdata2[0:nx]))))\n",
    "#         df2_1['y_ProductCodes'].append(','.join(list(chain.from_iterable(tmpdata2[nx:nx+ny]))))\n",
    "#     if ii % 10000 == 0:\n",
    "#         duration = datetime.datetime.now()-start;m, s = divmod(duration.seconds, 60)\n",
    "#         h, m = divmod(m, 60);print(\"[%d]/[%d]-----[%02d:%02d:%02d]\" %(ii, len(df2)-nx, h, m, s)) \n",
    "# df3 =pd.merge(\n",
    "#     pd.DataFrame(df2_1), \n",
    "#     df1[['BpID','Region','BpType','BpClass','ConstructionType']].drop_duplicates(keep='first'),\n",
    "#     on = 'BpID')\n",
    "\n",
    "# # 이상 데이터 전처리\n",
    "# # df3.to_csv('./Datasets/df3.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터 전처리 이후부터 작업\n",
    "# df3 = pd.read_csv('./Datasets/df3.csv', dtype='str', encoding = 'utf-8')\n",
    "\n",
    "# # item multihot vectorizer\n",
    "# x_item_embed = ItemOneHotGenerator(df3.x_ProductCodes, threshold=50) # 30 이상 구매건#######\n",
    "\n",
    "# # n구매이력아이템 제거->row정리in X_data, x_item_embed\n",
    "# drop_target_index = x_item_embed[x_item_embed.sum(axis = 1) ==0].index\n",
    "\n",
    "# X_data = df3.drop(drop_target_index, axis = 0).reset_index(drop=True)\n",
    "# x_item_embed = x_item_embed.drop(drop_target_index, axis = 0).reset_index(drop=True)\n",
    "\n",
    "# # 크로스 컬럼 생성\n",
    "# x_cross_embed = CrossColumnGenerator(X_data.x_ProductCodes, threshold=200) #100 이상 공동구매##### \n",
    "\n",
    "# # print(' X_data:', X_data.shape,  '\\n', 'x_item:', x_item_embed.shape, '\\n', \n",
    "# #       'x_cross:', x_cross_embed.shape)\n",
    "\n",
    "# # # Doc2Vec 임베딩 생성\n",
    "# x_d2v_embed = myDoc2Vec(data=X_data[['BpID','x_ProductCodes']], d2v_size=300, trainTF=False,\n",
    "#                         model_loc='./Trained_models/Models_Doc2Vec/BpID_300_181022.model')\n",
    "# # print('x_d2v:', x_d2v_embed.shape)\n",
    "\n",
    "# # Label 처리\n",
    "# label_1 = [ii.split(',') for ii in X_data.y_ProductCodes]\n",
    "\n",
    "# buy_cnt = []\n",
    "# buy_items = []\n",
    "# for ii, jj in pd.Series(label_1).iteritems():\n",
    "#     buy_cnt.append(len(jj))\n",
    "#     buy_items.extend(jj)\n",
    "\n",
    "# label_df = pd.DataFrame(buy_items, columns = ['label'])\n",
    "# # 1 label - multiple x's\n",
    "# X_data = X_data.drop(['BpID','x_ProductCodes','y_ProductCodes'], axis = 1)\n",
    "# X_data_1=pd.DataFrame(np.repeat(X_data.values,buy_cnt,axis=0),columns=X_data.columns)\n",
    "# x_item_embed_1=pd.DataFrame(np.repeat(x_item_embed.values,buy_cnt,axis=0),columns=x_item_embed.columns)\n",
    "# x_cross_embed_1=pd.DataFrame(np.repeat(x_cross_embed.values,buy_cnt,axis=0),columns=x_cross_embed.columns)\n",
    "# x_d2v_embed_1=pd.DataFrame(np.repeat(x_d2v_embed.values,buy_cnt,axis=0),columns=x_d2v_embed.columns)\n",
    "\n",
    "# # print(X_data_1.shape)\n",
    "# # print(x_item_embed_1.shape)\n",
    "# # print(x_cross_embed_1.shape)\n",
    "# # print(x_d2v_embed_1.shape)\n",
    "# # print(label_df.shape)\n",
    "\n",
    "# # 레이블의 비대칭성 개선 --- y값 threshold 적용\n",
    "# y_threshold = 10 # 10번 미만 구매 이력의 경우 제외\n",
    "# y_target1 = label_df.groupby('label')['label'].count().sort_values(ascending = False)\n",
    "# y_target2 = y_target1[y_target1.values>=y_threshold]\n",
    "# y_target3 = label_df.label.isin(y_target2.index)\n",
    "\n",
    "# label_df = label_df[y_target3].reset_index(drop=True)\n",
    "# X_data_1 = X_data_1[y_target3].reset_index(drop=True)\n",
    "# x_item_embed_1 = x_item_embed_1[y_target3].reset_index(drop=True)\n",
    "# x_cross_embed_1 = x_cross_embed_1[y_target3].reset_index(drop=True)\n",
    "# x_d2v_embed_1 = x_d2v_embed_1[y_target3].reset_index(drop=True)\n",
    "\n",
    "# # print(X_data_1.shape)\n",
    "# # print(x_item_embed_1.shape)\n",
    "# # print(x_cross_embed_1.shape)\n",
    "# # print(x_d2v_embed_1.shape)\n",
    "# # print(label_df.shape)\n",
    "\n",
    "# # Label encoding -> all strings\n",
    "# le_BpType = LabelEncoder()\n",
    "# BpType_embed = le_BpType.fit_transform(X_data_1.BpType.tolist())\n",
    "# with open('./le_BpType.pkl','wb') as f:\n",
    "#     pickle.dump(le_BpType, f)\n",
    "\n",
    "# le_BpClass = LabelEncoder()\n",
    "# BpClass_embed = le_BpClass.fit_transform(X_data_1.BpClass.tolist())\n",
    "# with open('./le_BpClass.pkl','wb') as g:\n",
    "#     pickle.dump(le_BpClass, g)\n",
    "    \n",
    "# X_data_1['BpType'] = np.core.defchararray.add('bt', BpType_embed.astype('str'))\n",
    "# X_data_1['BpClass'] =np.core.defchararray.add('bc', BpClass_embed.astype('str'))\n",
    "# X_data_1['Region'] =np.core.defchararray.add('rg', X_data_1['Region'].values.astype('str'))\n",
    "# X_data_1['ConstructionType'] =np.core.defchararray.add('ct', X_data_1['ConstructionType'].values.astype('str'))\n",
    "\n",
    "# # csv 디폴트 컬럼 값 만들기\n",
    "# np_matrix = pd.concat([X_data_1.iloc[:1,:],x_item_embed_1.iloc[:1,:],\n",
    "#                        x_cross_embed_1.iloc[:1,:],x_d2v_embed_1.iloc[:1,:],\n",
    "#                        label_df.iloc[:1,:]], axis =1)\n",
    "# _csv_col_defaults = []\n",
    "# for ii in np_matrix.dtypes:\n",
    "#     if ii == 'object':\n",
    "#         _csv_col_defaults.append([\"\"])\n",
    "#     elif ii == 'int64':\n",
    "#         _csv_col_defaults.append([0])\n",
    "#     elif ii == 'float64':\n",
    "#         _csv_col_defaults.append([0.0])\n",
    "#     else:\n",
    "#         continue\n",
    "        \n",
    "# iK = {\n",
    "#     'df_size':int(1e4),\n",
    "#     'df_cnt':int(label_df.shape[0]/int(1e4)),\n",
    "#     'featurenm':np_matrix.columns[:-1],\n",
    "#     'x_item_colnm':x_item_embed_1.columns,\n",
    "#     'x_cross_colnm':x_cross_embed_1.columns,\n",
    "#     'x_d2v_colnm':x_d2v_embed_1.columns,\n",
    "#     'x_else_colnm':X_data_1.columns,\n",
    "#     'ConstructionType':X_data_1.ConstructionType.unique(),\n",
    "#     'BpType':X_data_1.BpType.unique(),\n",
    "#     'BpClass':X_data_1.BpClass.unique(),\n",
    "#     'Region':X_data_1.Region.unique(),\n",
    "#     'y_classes':tuple(label_df.label.unique()),\n",
    "#     '_csv_col_defaults':_csv_col_defaults\n",
    "# }\n",
    "# with open('./iKeys.pkl','wb') as f:\n",
    "#     pickle.dump(iK, f)\n",
    "    \n",
    "# # Slice and save_CSV files\n",
    "# for ii in range(iK['df_cnt']):\n",
    "#     filenm = \"./Datasets/dataset\"+str(ii)+\".csv\"\n",
    "#     if ii == iK['df_cnt']-1:\n",
    "#         np_matrix = pd.concat([\n",
    "#             X_data_1.iloc[ii*iK['df_size']:,:],\n",
    "#             x_item_embed_1.iloc[ii*iK['df_size']:,:],\n",
    "#             x_cross_embed_1.iloc[ii*iK['df_size']:,:],\n",
    "#             x_d2v_embed_1.iloc[ii*iK['df_size']:,:],\n",
    "#             label_df.iloc[ii*iK['df_size']:,:]], axis =1).values\n",
    "#     else:\n",
    "#         np_matrix = pd.concat([\n",
    "#             X_data_1.iloc[ii*iK['df_size']:ii*iK['df_size']+iK['df_size'],:],\n",
    "#             x_item_embed_1.iloc[ii*iK['df_size']:ii*iK['df_size']+iK['df_size'],:],\n",
    "#             x_cross_embed_1.iloc[ii*iK['df_size']:ii*iK['df_size']+iK['df_size'],:],\n",
    "#             x_d2v_embed_1.iloc[ii*iK['df_size']:ii*iK['df_size']+iK['df_size'],:],\n",
    "#             label_df.iloc[ii*iK['df_size']:ii*iK['df_size']+iK['df_size'],:]], axis =1).values\n",
    "#     np.savetxt(fname=filenm, X=np_matrix, delimiter=',',encoding='utf-8',fmt=\"%s\")\n",
    "#     print(\"[%d]/[%d]\" %(ii, iK['df_cnt']))\n",
    "# print('CSV-OUT JOB DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델평가\n",
    "# 여기서부터 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['df_size', 'featurenm', 'x_cross_colnm', 'Region', 'y_classes', 'x_item_colnm', 'x_d2v_colnm', 'BpClass', 'x_else_colnm', 'BpType', '_csv_col_defaults', 'df_cnt', 'ConstructionType'])\n"
     ]
    }
   ],
   "source": [
    "import shutil, os, warnings, re, time, itertools, datetime, multiprocessing\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from itertools import combinations\n",
    "from gensim.models import Doc2Vec\n",
    "from collections import namedtuple, Counter\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "warnings.filterwarnings(action='default')\n",
    "\n",
    "model_dir = \"./Trained_models/Models_WnD/m1031/\"\n",
    "    \n",
    "with open('./iKeys.pkl','rb') as f:\n",
    "    iK = pickle.load(f)\n",
    "print(iK.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(data_files, num_epochs, batch_size, predict=False):\n",
    "    def parse_csv(value):\n",
    "        columns = tf.decode_csv(value, record_defaults=iK['_csv_col_defaults'])\n",
    "        f_str = dict(zip(iK['featurenm'][:4],columns[:4]))\n",
    "        f_int = {'item_cross':tf.stack(columns[4:-301],axis=-1)}#item + cross\n",
    "        f_d2v = {'d2v': tf.stack(columns[-301:-1],axis=-1)}\n",
    "        label = columns[-1]\n",
    "        out = {**f_str, **f_int, **f_d2v}, label\n",
    "        return out\n",
    "    \n",
    "    dataset = tf.data.TextLineDataset(data_files)\n",
    "    if predict==False:\n",
    "        dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(buffer_size = 400000, count = num_epochs))\n",
    "    dataset = dataset.apply(tf.contrib.data.map_and_batch(map_func=parse_csv, \n",
    "                                                          batch_size=batch_size, \n",
    "                                                          num_parallel_calls = 16))\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(buffer_size = 50)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files=== \n",
      " ['./Datasets/dataset1.csv', './Datasets/dataset2.csv', './Datasets/dataset3.csv', './Datasets/dataset4.csv', './Datasets/dataset6.csv', './Datasets/dataset7.csv', './Datasets/dataset8.csv', './Datasets/dataset9.csv', './Datasets/dataset11.csv', './Datasets/dataset12.csv', './Datasets/dataset13.csv', './Datasets/dataset14.csv', './Datasets/dataset16.csv', './Datasets/dataset17.csv', './Datasets/dataset18.csv', './Datasets/dataset19.csv', './Datasets/dataset21.csv', './Datasets/dataset22.csv', './Datasets/dataset23.csv', './Datasets/dataset24.csv', './Datasets/dataset26.csv', './Datasets/dataset27.csv', './Datasets/dataset28.csv', './Datasets/dataset29.csv', './Datasets/dataset31.csv', './Datasets/dataset32.csv', './Datasets/dataset33.csv', './Datasets/dataset34.csv', './Datasets/dataset36.csv', './Datasets/dataset37.csv', './Datasets/dataset38.csv', './Datasets/dataset39.csv', './Datasets/dataset41.csv']\n",
      "Eval files=== \n",
      " ['./Datasets/dataset0.csv', './Datasets/dataset5.csv', './Datasets/dataset10.csv', './Datasets/dataset15.csv', './Datasets/dataset20.csv', './Datasets/dataset25.csv', './Datasets/dataset30.csv', './Datasets/dataset35.csv', './Datasets/dataset40.csv']\n",
      "Predict files=== \n",
      " ['./Datasets/dataset0.csv', './Datasets/dataset5.csv', './Datasets/dataset10.csv']\n"
     ]
    }
   ],
   "source": [
    "filenames = [\"./Datasets/\"+f_name for f_name in os.listdir('./Datasets/') if f_name.startswith('dataset') and f_name.endswith('.csv')]\n",
    "filename_split = train_eval_filesplit(filenames = filenames, train_rate=0.8)\n",
    "predict_filenames = filename_split['eval_files'][:3]\n",
    "\n",
    "def train_input_fn():\n",
    "    return input_fn(data_files=filename_split['train_files'],num_epochs=int(1e6), batch_size=64)\n",
    "\n",
    "def eval_input_fn():\n",
    "    return input_fn(data_files=filename_split['eval_files'], num_epochs=1, batch_size=1)\n",
    "\n",
    "def pred_input_fn():\n",
    "    return input_fn(data_files=predict_filenames,num_epochs=1,batch_size=1,predict=True)\n",
    "\n",
    "print(\"Train files===\",\"\\n\",filename_split['train_files'])\n",
    "print(\"Eval files===\",\"\\n\",filename_split['eval_files'])\n",
    "print(\"Predict files===\",\"\\n\",predict_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/tf_logging.py:120: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  _get_logger().warn(msg, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "item_dim = len(iK['x_item_colnm']) + len(iK['x_cross_colnm'])\n",
    "item_cross = tf.contrib.layers.real_valued_column(\"item_cross\", dimension=item_dim, dtype=tf.int64)\n",
    "item_cross_buckets = tf.contrib.layers.bucketized_column(item_cross, boundaries=[1])\n",
    "\n",
    "d2v = tf.contrib.layers.real_valued_column(\"d2v\", dimension=300, dtype=tf.float64)\n",
    "\n",
    "BpClass=tf.contrib.layers.sparse_column_with_hash_bucket(\"BpClass\", hash_bucket_size=300)\n",
    "BpType=tf.contrib.layers.sparse_column_with_hash_bucket(\"BpType\", hash_bucket_size=300)\n",
    "ConstructionType=tf.contrib.layers.sparse_column_with_hash_bucket(\"ConstructionType\", hash_bucket_size=100)\n",
    "Region=tf.contrib.layers.sparse_column_with_hash_bucket(\"Region\", hash_bucket_size=100)\n",
    "\n",
    "wide_columns = [\n",
    "    item_cross_buckets, \n",
    "    ConstructionType]\n",
    "deep_columns = [\n",
    "    d2v, \n",
    "    tf.contrib.layers.embedding_column(BpClass, dimension=128), \n",
    "    tf.contrib.layers.embedding_column(BpType, dimension=128), \n",
    "    tf.contrib.layers.embedding_column(ConstructionType, dimension=128), \n",
    "    tf.contrib.layers.embedding_column(Region, dimension=128)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model config setup for speed up (reference : tensorflow performance guide)\n",
    "config = tf.ConfigProto()\n",
    "config.intra_op_parallelism_threads = 32 # 8->16->32->64....\n",
    "config.inter_op_parallelism_threads = 0 # 0으로 고정\n",
    "_config = tf.contrib.learn.RunConfig(\n",
    "    session_config=config, \n",
    "    save_checkpoints_secs=None, \n",
    "    save_checkpoints_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_task_type': None, '_keep_checkpoint_max': 5, '_environment': 'local', '_eval_distribute': None, '_save_summary_steps': 100, '_task_id': 0, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 32\n",
      ", '_model_dir': './Trained_models/Models_WnD/m1031/', '_protocol': None, '_num_worker_replicas': 0, '_master': '', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7e501ba860>, '_num_ps_replicas': 0, '_train_distribute': None, '_log_step_count_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_steps': 1000, '_evaluation_master': '', '_device_fn': None, '_tf_random_seed': None, '_is_chief': True, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "tot_model = tf.estimator.DNNLinearCombinedClassifier(\n",
    "    model_dir=model_dir, \n",
    "    linear_feature_columns=wide_columns, \n",
    "    dnn_feature_columns=deep_columns, \n",
    "    dnn_hidden_units= [1000, 500, 200], dnn_dropout = 0.5, dnn_optimizer= tf.train.AdamOptimizer,\n",
    "    n_classes = len(iK['y_classes']), label_vocabulary=iK['y_classes'], batch_norm = True,\n",
    "    config = _config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedClassifier\n",
    "p_result = tot_model.predict(input_fn=pred_input_fn, yield_single_examples=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Datasets/dataset0.csv is in progress...\n",
      "./Datasets/dataset5.csv is in progress...\n",
      "./Datasets/dataset10.csv is in progress...\n",
      "process done\n"
     ]
    }
   ],
   "source": [
    "true_label = []\n",
    "for ii in range(len(predict_filenames)):\n",
    "    print(predict_filenames[ii],'is in progress...')\n",
    "    tmp = np.loadtxt(fname=predict_filenames[ii],delimiter=',',dtype='str')\n",
    "    true_label.extend(tmp[:,-1].tolist())\n",
    "print('process done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./Trained_models/Models_WnD/m1031/model.ckpt-0\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey dnn/hiddenlayer_2/batchnorm_2/beta not found in checkpoint\n\t [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2817, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2843, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3018, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3183, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3265, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-174-0458ed75f68b>\", line 2, in <module>\n    for num, item in enumerate(p_result):\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 567, in predict\n    hooks=all_hooks) as mon_sess:\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 921, in __init__\n    stop_grace_period_secs=stop_grace_period_secs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 643, in __init__\n    self._sess = _RecoverableSession(self._coordinated_creator)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1107, in __init__\n    _WrappedSession.__init__(self, self._create_session())\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1112, in _create_session\n    return self._sess_creator.create_session()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 800, in create_session\n    self.tf_sess = self._session_creator.create_session()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 557, in create_session\n    self._scaffold.finalize()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 213, in finalize\n    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 878, in _get_saver_or_default\n    saver = Saver(sharded=True, allow_empty=True)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1094, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1106, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1143, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 781, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 459, in _AddShardedRestoreOps\n    name=\"restore_shard\"))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 406, in _AddRestoreOps\n    restore_sequentially)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 854, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1466, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nNotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey dnn/hiddenlayer_2/batchnorm_2/beta not found in checkpoint\n\t [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key dnn/hiddenlayer_2/batchnorm_2/beta not found in checkpoint\n\t [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1538\u001b[0;31m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1539\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key dnn/hiddenlayer_2/batchnorm_2/beta not found in checkpoint\n\t [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2817, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2843, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3018, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3183, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3265, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-174-0458ed75f68b>\", line 2, in <module>\n    for num, item in enumerate(p_result):\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 567, in predict\n    hooks=all_hooks) as mon_sess:\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 921, in __init__\n    stop_grace_period_secs=stop_grace_period_secs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 643, in __init__\n    self._sess = _RecoverableSession(self._coordinated_creator)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1107, in __init__\n    _WrappedSession.__init__(self, self._create_session())\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1112, in _create_session\n    return self._sess_creator.create_session()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 800, in create_session\n    self.tf_sess = self._session_creator.create_session()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 557, in create_session\n    self._scaffold.finalize()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 213, in finalize\n    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 878, in _get_saver_or_default\n    saver = Saver(sharded=True, allow_empty=True)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1094, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1106, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1143, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 781, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 459, in _AddShardedRestoreOps\n    name=\"restore_shard\"))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 406, in _AddRestoreOps\n    restore_sequentially)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 854, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1466, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nNotFoundError (see above for traceback): Key dnn/hiddenlayer_2/batchnorm_2/beta not found in checkpoint\n\t [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1547\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1548\u001b[0;31m         \u001b[0mnames_to_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_graph_key_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1549\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mobject_graph_key_mapping\u001b[0;34m(checkpoint_path)\u001b[0m\n\u001b[1;32m   1821\u001b[0m   object_graph_string = reader.get_tensor(\n\u001b[0;32m-> 1822\u001b[0;31m       checkpointable.OBJECT_GRAPH_PROTO_KEY)\n\u001b[0m\u001b[1;32m   1823\u001b[0m   object_graph_proto = (\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mget_tensor\u001b[0;34m(self, tensor_str)\u001b[0m\n\u001b[1;32m    358\u001b[0m         return CheckpointReader_GetTensor(self, compat.as_bytes(tensor_str),\n\u001b[0;32m--> 359\u001b[0;31m                                           status)\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    527\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-174-0458ed75f68b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mp_result_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[1;32m    565\u001b[0m                 \u001b[0mscaffold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaffold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                 config=self._session_config),\n\u001b[0;32m--> 567\u001b[0;31m             hooks=all_hooks) as mon_sess:\n\u001b[0m\u001b[1;32m    568\u001b[0m           \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0mpreds_evaluated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m    919\u001b[0m     super(MonitoredSession, self).__init__(\n\u001b[1;32m    920\u001b[0m         \u001b[0msession_creator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, should_recover, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_RecoverableSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess_creator)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \"\"\"\n\u001b[1;32m   1106\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess_creator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m     \u001b[0m_WrappedSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m_create_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m         logging.info('An error was raised while a session was being created. '\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mcreate_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    798\u001b[0m       \u001b[0;34m\"\"\"Creates a coordinated session.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m       \u001b[0;31m# Keep the tf_sess for unit testing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m       \u001b[0;31m# We don't want coordinator to suppress any exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCoordinator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_stop_exception_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mcreate_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0minit_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0minit_feed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_feed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m         init_fn=self._scaffold.init_fn)\n\u001b[0m\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/session_manager.py\u001b[0m in \u001b[0;36mprepare_session\u001b[0;34m(self, master, init_op, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config, init_feed_dict, init_fn)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mwait_for_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait_for_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         config=config)\n\u001b[0m\u001b[1;32m    282\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_loaded_from_checkpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minit_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minit_fn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_init_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/session_manager.py\u001b[0m in \u001b[0;36m_restore_checkpoint\u001b[0;34m(self, master, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheckpoint_filename_with_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_filename_with_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;31m# a helpful message (b/110263146)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m         raise _wrap_restore_error_with_msg(\n\u001b[0;32m-> 1554\u001b[0;31m             err, \"a Variable name or other graph key that is missing\")\n\u001b[0m\u001b[1;32m   1555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m       \u001b[0;31m# This is an object-based checkpoint. We'll print a warning and then do\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey dnn/hiddenlayer_2/batchnorm_2/beta not found in checkpoint\n\t [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2817, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2843, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3018, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3183, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3265, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-174-0458ed75f68b>\", line 2, in <module>\n    for num, item in enumerate(p_result):\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 567, in predict\n    hooks=all_hooks) as mon_sess:\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 921, in __init__\n    stop_grace_period_secs=stop_grace_period_secs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 643, in __init__\n    self._sess = _RecoverableSession(self._coordinated_creator)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1107, in __init__\n    _WrappedSession.__init__(self, self._create_session())\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1112, in _create_session\n    return self._sess_creator.create_session()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 800, in create_session\n    self.tf_sess = self._session_creator.create_session()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 557, in create_session\n    self._scaffold.finalize()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 213, in finalize\n    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 878, in _get_saver_or_default\n    saver = Saver(sharded=True, allow_empty=True)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1094, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1106, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1143, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 781, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 459, in _AddShardedRestoreOps\n    name=\"restore_shard\"))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 406, in _AddRestoreOps\n    restore_sequentially)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 854, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1466, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nNotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey dnn/hiddenlayer_2/batchnorm_2/beta not found in checkpoint\n\t [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "p_result_final = []\n",
    "for num, item in enumerate(p_result):\n",
    "    if num%10000==0:\n",
    "        print(num)\n",
    "    if num>=len(true_label):\n",
    "        break\n",
    "    else:\n",
    "        p_result_final.append(item['probabilities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-169-041adab53335>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-169-041adab53335>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    print(\"적중률 : %.2f\",%(np.mean(calc_rate)))\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "list_TopN=[3,5,10]\n",
    "for ii in range(len(list_TopN)):\n",
    "    TopN = list_TopN[ii]\n",
    "    calc_rate = []\n",
    "    for ii in range(len(true_label)):\n",
    "        sort_idx = (np.argsort(np.argsort(p_result_final[ii]))>=(len(p_result_final[ii][0])-TopN)).reshape(-1)\n",
    "        calc_rate.append(int(true_label[ii] in np.array(iK['y_classes'])[sort_idx].tolist()))\n",
    "\n",
    "    print(\"적중률 : %.2f\",%(np.mean(calc_rate)))\n",
    "    print(\"[%d]개를 추천하면 그 중에 [%.2f]개를 구매함\" %(TopN, np.mean(calc_rate)*TopN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 181030 - m1026_iter56000모델로 예측한 결과------------------------\n",
    "# 적중률 :  0.0817\n",
    "# [3]개를 추천하면 그 중에 [0.25]개를 구매함\n",
    "# 적중률 :  0.1241\n",
    "# [5]개를 추천하면 그 중에 [0.62]개를 구매함\n",
    "# 적중률 :  0.1966\n",
    "# [10]개를 추천하면 그 중에 [1.97]개를 구매함\n",
    "# 그러나 X에 대한 진짜 답이 p_result인지 아직 모른다.\n",
    "# 맞다고 한다면 아직은 절망적인 성능을 보임.\n",
    "\n",
    "# 181030 - m1029_iter12000모델로 예측한 결과------------------------\n",
    "# 적중률 :  0.074\n",
    "# [3]개를 추천하면 그 중에 [0.22]개를 구매함\n",
    "# 적중률 :  0.1099\n",
    "# [5]개를 추천하면 그 중에 [0.55]개를 구매함\n",
    "# 적중률 :  0.1788\n",
    "# [10]개를 추천하면 그 중에 [1.79]개를 구매함\n",
    "# 그러나 X에 대한 진짜 답이 p_result인지 아직 모른다.\n",
    "# 맞다고 한다면 아직은 절망적인 성능을 보임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 정답 불러오기.\n",
    "# eval_f= filename_split['eval_files']\n",
    "# for ii in range(len(eval_f)):\n",
    "#     print(ii, len(eval_f)-1)\n",
    "#     tmp = np.loadtxt(fname=eval_f[ii],delimiter=',',dtype='str')\n",
    "#     if ii == 0:\n",
    "#         home1 = tmp.copy()\n",
    "#     else:\n",
    "#         home1 = np.concatenate((home1,tmp), axis =0)\n",
    "# print(home1.shape)\n",
    "# # 정상확인되면 나중에는 y값만 가져오도록 구정해도됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
