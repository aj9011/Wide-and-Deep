{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os, warnings, re, datetime, multiprocessing\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import _pickle as pickle\n",
    "from gensim.models import Doc2Vec\n",
    "from itertools import chain, combinations, permutations, product, combinations_with_replacement\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import namedtuple, Counter\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "warnings.filterwarnings(action='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ItemOneHotGenerator(data, threshold = 30):\n",
    "    \"\"\"\n",
    "    data : Data format must be pandas Series with rows having list of items.\n",
    "              reset_index() required.\n",
    "              example : '6601000010,6601000024,....,6601000001'\n",
    "    threshold : To exclude super sparse case of cross columns\n",
    "    dependencies : collections.Counter\n",
    "                              sklearn.feature_extraction.text.CountVectorizer\n",
    "    \"\"\"    \n",
    "    cross_cnt = Counter()\n",
    "    for ii, jj in data.iteritems():\n",
    "        litem = sorted(set(jj.split(',')))\n",
    "        cross_cnt.update(litem)\n",
    "    target_lcross = set({x : cross_cnt[x] for x in cross_cnt if cross_cnt[x] > threshold}.keys()) \n",
    "    \n",
    "    cross_feature = []\n",
    "    for ii, jj in data.iteritems():\n",
    "        litem = sorted(set(jj.split(',')))\n",
    "        lcross = set(litem).intersection(target_lcross)\n",
    "        cross_feature.append(lcross)\n",
    "        \n",
    "    lcross_feature = map(lambda x:' '.join(x), cross_feature)\n",
    "    vect = CountVectorizer(tokenizer=str.split)\n",
    "    mcross_feature = vect.fit_transform(lcross_feature)\n",
    "    with open('./ItemOneHotGenerator.pkl','wb') as f:\n",
    "        pickle.dump(vect, f)\n",
    "    return pd.DataFrame(mcross_feature.todense(), columns=vect.get_feature_names())\n",
    "\n",
    "def CrossColumnGenerator(data, threshold = 5):\n",
    "    \"\"\"\n",
    "    data : Data format must be pandas Series with rows having list of items.\n",
    "              reset_index() required.\n",
    "              example : '6601000010,6601000024,....,6601000001'\n",
    "    threshold : To exclude super sparse case of cross columns\n",
    "    dependencies : collections.Counter\n",
    "                              sklearn.feature_extraction.text.CountVectorizer\n",
    "    \"\"\"\n",
    "    cross_cnt = Counter()\n",
    "    for ii, jj in data.iteritems():\n",
    "        litem = sorted(set(jj.split(',')))\n",
    "        cross_cnt.update([str(kk + '_' + ll) for kk, ll in combinations(litem, 2)])\n",
    "        \n",
    "    target_lcross = set({x : cross_cnt[x] for x in cross_cnt if cross_cnt[x] >= threshold}.keys()) \n",
    "    \n",
    "    cross_feature = []\n",
    "    for ii, jj in data.iteritems():\n",
    "        litem = sorted(set(jj.split(',')))\n",
    "        lcross = set([str(kk + '_' + ll) for kk, ll in combinations(litem, 2)]).intersection(target_lcross)\n",
    "        cross_feature.append(lcross)\n",
    "\n",
    "    lcross_feature = map(lambda x:' '.join(x), cross_feature)\n",
    "    vect = CountVectorizer(tokenizer=str.split)\n",
    "    mcross_feature = vect.fit_transform(lcross_feature)\n",
    "    with open('./CrossColumnGenerator.pkl','wb') as f:\n",
    "        pickle.dump(vect, f)\n",
    "    return pd.DataFrame(mcross_feature.todense(), columns=vect.get_feature_names())\n",
    "\n",
    "def myDoc2Vec(data, d2v_size, model_loc, epoch=None, trainTF = False):\n",
    "    \"\"\"\n",
    "    data : DataFrame([['DocID','Item']])\n",
    "    \"\"\"\n",
    "    data.columns = ['docID','item']\n",
    "    data1 = [(str(row['item']).split(), row['docID']) for idx, row in data.iterrows()]\n",
    "    data2 = namedtuple('TaggedDocument', 'words tags')\n",
    "    tagged_data2 = [data2(d, [c]) for d, c in data1]\n",
    "    \n",
    "    if trainTF:\n",
    "        model = Doc2Vec(\n",
    "            dm = 0,  # 0 : PV-DBOW\n",
    "            dbow_words = 0,  # 0 : train doc-vec only(faster)\n",
    "            window = 8, vector_size = d2v_size, alpha = 0.025, min_alpha = 0.025, seed = 0, sample= 1e-5, min_count=3, \n",
    "            workers=multiprocessing.cpu_count(), hs = 0, negative = 10)\n",
    "        model.build_vocab(tagged_data2)\n",
    "        print('New model training started')\n",
    "        model.train(documents  = tagged_data2, total_examples = data.shape[0], epochs = epoch)\n",
    "        model.save(model_loc)\n",
    "        print('New model training done. check--',model_loc)\n",
    "    else:\n",
    "        model=Doc2Vec.load(model_loc)\n",
    "        embedding_df = pd.DataFrame(\n",
    "            data = [model.infer_vector(doc.words) for doc in tagged_data2], \n",
    "            columns = [\"d2v\"+str(i) for i in range(d2v_size)])\n",
    "        return embedding_df\n",
    "    \n",
    "def train_eval_filesplit(filenames, train_rate = 0.8):\n",
    "    trainfiles = []\n",
    "    evalfiles = []\n",
    "    filecnt = len(filenames)\n",
    "    trainloop = int(round(filecnt/(filecnt - int(filecnt*train_rate)),0))\n",
    "    for ii in range(filecnt):\n",
    "        if ii % trainloop == 0:\n",
    "            evalfiles.append(filenames[ii])\n",
    "        else:\n",
    "            trainfiles.append(filenames[ii])\n",
    "    filedict = {'train_files':trainfiles,'eval_files':evalfiles}\n",
    "    return filedict\n",
    "\n",
    "def custom_evaluation():\n",
    "    print('Start Target Label Importing')\n",
    "    true_label = []\n",
    "    for ii in range(len(predict_filenames)):\n",
    "        print(predict_filenames[ii],'is in progress...')\n",
    "        tmp = np.loadtxt(fname=predict_filenames[ii],delimiter=',',dtype='str')\n",
    "        true_label.extend(tmp[:,-1].tolist())\n",
    "    print('Target Label Import Process DONE')\n",
    "\n",
    "    print('Start Prediction')\n",
    "    p_result = tot_model.predict(input_fn=pred_input_fn, yield_single_examples=False)\n",
    "    p_result_final = []\n",
    "    for num, item in enumerate(p_result):\n",
    "        if num%10000==0:\n",
    "            print(num)\n",
    "        if num>=len(true_label):\n",
    "            break\n",
    "        else:\n",
    "            p_result_final.append(item['probabilities'])\n",
    "    print('Labe Prediction Process DONE')\n",
    "\n",
    "    print('Start 3, 5, 10 recommendation scenario evaluation')\n",
    "    list_TopN=[3,5,10]\n",
    "    for ii in range(len(list_TopN)):\n",
    "        TopN = list_TopN[ii]\n",
    "        calc_rate = []\n",
    "        for ii in range(len(true_label)):\n",
    "            sort_idx = (np.argsort(np.argsort(p_result_final[ii]))>=(len(p_result_final[ii][0])-TopN)).reshape(-1)\n",
    "            calc_rate.append(int(true_label[ii] in np.array(iK['y_classes'])[sort_idx].tolist()))\n",
    "\n",
    "        print(\"Accuracy : %.2f\" %(np.mean(calc_rate)))\n",
    "        print(\"[%d]개를 추천하면 그 중에 [%.2f]개를 구매함\" %(TopN, np.mean(calc_rate)*TopN))\n",
    "    print('Scenario Evaluation Process DONE')\n",
    "    \n",
    "def input_fn(data_files, batch_size, predict=False): #\n",
    "    def parse_csv(value):\n",
    "        columns = tf.decode_csv(value, record_defaults=[[\"\"]]*6+[[0]])\n",
    "        label = columns[-1]\n",
    "        del columns[-1]\n",
    "        features = dict(zip(df4.columns.tolist()[:-1],columns))\n",
    "        out = features, label\n",
    "        return out\n",
    "    \n",
    "    dataset = tf.data.TextLineDataset(data_files)\n",
    "    if predict==False:\n",
    "        dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(buffer_size = 100))\n",
    "    dataset = dataset.apply(tf.contrib.data.map_and_batch(map_func=parse_csv, \n",
    "                                                          batch_size=batch_size, \n",
    "                                                          num_parallel_calls = 16))\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(buffer_size = 50)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "주문_2014 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문_2014.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "주문_2015 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문_2015.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "주문_2016 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문_2016.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "주문_2017 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문_2017.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "주문_2018 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문_2018.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "주문상품_2014 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문상품_2014.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "주문상품_2015 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문상품_2015.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "주문상품_2016 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문상품_2016.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "주문상품_2017 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문상품_2017.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "주문상품_2018 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문상품_2018.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "주문상품출하_2014 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문상품출하_2014.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "주문상품출하_2015 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문상품출하_2015.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "주문상품출하_2016 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문상품출하_2016.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "주문상품출하_2017 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문상품출하_2017.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "주문상품출하_2018 = pd.read_csv('C:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가5차_180920/주문상품출하_2018.csv', encoding = 'utf-8', index_col=0, dtype = 'str')\n",
    "상품마스터 = pd.read_csv('c:/Users/kim85/OneDrive/Project_AgileSoda/원본/추가6차_181005/상품마스터.csv',encoding ='utf-8',dtype = 'str')\n",
    "조직마스터 = pd.read_csv('c:/users/kim85/OneDrive/Project_AgileSoda/원본/추가6차_181005/조직마스터.csv', encoding ='utf-8', dtype ='str')\n",
    "사업장= pd.read_csv('c:/users/kim85/OneDrive/Project_AgileSoda/원본/추가6차_181005/사업장.csv', encoding ='utf-8', dtype ='str')\n",
    "공사유형 = pd.read_csv('c:/users/kim85/OneDrive/Project_AgileSoda/원본/추가6차_181005/공사유형.csv', encoding ='utf-8', dtype ='str')\n",
    "\n",
    "주문 = pd.concat([주문_2014, 주문_2015, 주문_2016, 주문_2017, 주문_2018])\n",
    "주문상품 = pd.concat([주문상품_2014, 주문상품_2015, 주문상품_2016, 주문상품_2017, 주문상품_2018])\n",
    "주문상품출하 = pd.concat([주문상품출하_2014, 주문상품출하_2015, 주문상품출하_2016, 주문상품출하_2017, 주문상품출하_2018])\n",
    "\n",
    "# 주문 처리\n",
    "주문 = 주문.reset_index()[['ORDE_IDEN_NUMB','CONS_IDEN_NAME','GROUPID','CLIENTID','BRANCHID',\n",
    "                       'DELI_AREA_CODE','REGI_DATE_TIME','ORDE_USER_ID']].drop_duplicates(keep='first')\n",
    "주문상품 = 주문상품.reset_index()[['ORDE_IDEN_NUMB','ORDE_SEQU_NUMB','GOOD_IDEN_NUMB',\n",
    "                           'ORDE_REQU_QUAN']].drop_duplicates(keep='first')\n",
    "주문상품 = 주문상품.groupby(['ORDE_IDEN_NUMB','GOOD_IDEN_NUMB'])['ORDE_REQU_QUAN'].agg('sum').reset_index()\n",
    "주문상품출하 =  주문상품출하.reset_index()[['ORDE_IDEN_NUMB', 'DELI_STAT_FLAG']].drop_duplicates(keep='first')\n",
    "주문상품출하 = 주문상품출하[주문상품출하.DELI_STAT_FLAG=='70'].ORDE_IDEN_NUMB.unique()\n",
    "\n",
    "주문상품_주문 = pd.merge(주문상품, 주문, on = 'ORDE_IDEN_NUMB', how = 'left')\n",
    "주문상품_주문_출하 = 주문상품_주문[주문상품_주문.ORDE_IDEN_NUMB.isin(주문상품출하)]\n",
    "주문전체 = 주문상품_주문_출하[주문상품_주문_출하.GROUPID != '101'].drop_duplicates(keep='first')\n",
    "\n",
    "# 상품 처리\n",
    "상품마스터 = 상품마스터[['good_iden_numb','cate_id','good_name','good_spec','good_type','repre_good']].drop_duplicates(keep='first')\n",
    "상품마스터 = 상품마스터.rename(columns = {'good_iden_numb':'GOOD_IDEN_NUMB'})\n",
    "# repre_good - Y : 옵션대표상품, N : 단품, P : 옵션상품 // good_type - 10 : 일반, 20 : 지정,60 : 공구, 70 : 안전,80 : 보안\n",
    "\n",
    "상품주문전체 = pd.merge(주문전체, 상품마스터, on = 'GOOD_IDEN_NUMB', how = 'left')\n",
    "상품주문전체 = 상품주문전체[상품주문전체.repre_good=='N'] # 옵션상품 제외\n",
    "\n",
    "# 사업장 처리\n",
    "사업장 = pd.concat([\n",
    "        사업장[['BRANCHID','AREATYPE','BRANCHBUSITYPE','BRANCHBUSICLAS','WORKID']].rename(columns={'BRANCHID':'BORGID'}), \n",
    "        사업장[['BRANCHCD','AREATYPE','BRANCHBUSITYPE','BRANCHBUSICLAS','WORKID']].rename(columns={'BRANCHCD':'BORGID'})\n",
    "    ], axis=0).drop_duplicates(keep='first')\n",
    "\n",
    "공사유형_사업장 = pd.merge(\n",
    "    사업장, \n",
    "    공사유형[['WORKID','WORKNM']].drop_duplicates(keep='first'), \n",
    "    how = 'left', on = 'WORKID')\n",
    "\n",
    "조직마스터 = 조직마스터[(조직마스터.BORGTYPECD == 'BCH') & (조직마스터.SVCTYPECD == 'BUY')] # 사업장레벨 및 구매사만\n",
    "조직마스터 = pd.concat([\n",
    "        조직마스터[['BORGID','BORGNM']], \n",
    "        조직마스터[['BORGCD','BORGNM']].rename(columns={'BORGCD':'BORGID'})\n",
    "    ], axis=0).drop_duplicates(keep='first')\n",
    "\n",
    "조직전체 = pd.merge(조직마스터, 공사유형_사업장, how = 'left', on = 'BORGID').rename(columns={'BORGID':'BRANCHID'})\n",
    "\n",
    "df = pd.merge(상품주문전체,조직전체,how = 'left',on = 'BRANCHID')\n",
    "# 불필요컬럼 제외\n",
    "df = df.drop([\n",
    "        'CONS_IDEN_NAME','GROUPID','CLIENTID','ORDE_USER_ID','good_name','good_spec','repre_good','WORKNM','BORGNM'\n",
    "        ], axis = 1)\n",
    "df = df.drop_duplicates(keep='first')\n",
    "# 날짜컬럼추가\n",
    "df[\"REGI_DATE\"] = pd.to_datetime(df.REGI_DATE_TIME).dt.date\n",
    "# 문자 -> 숫자화1\n",
    "bptype = LabelEncoder()\n",
    "df['BpType']=bptype.fit_transform(df.BRANCHBUSITYPE.tolist()).astype('str')\n",
    "with open('./le_BpType.pkl','wb') as f:\n",
    "    pickle.dump(bptype, f)\n",
    "# 문자 -> 숫자화2\n",
    "bpclass = LabelEncoder()\n",
    "df['BpClass']=bpclass.fit_transform(df.BRANCHBUSICLAS.tolist()).astype('str')\n",
    "with open('./le_BpClass.pkl','wb') as g:\n",
    "    pickle.dump(bpclass, g)\n",
    "# 널 데이터 제외(764건)\n",
    "df = df.dropna()\n",
    "df = df.drop(['BRANCHBUSITYPE','BRANCHBUSICLAS'],axis =1)\n",
    "df.columns = ['OrderNum','ProductCode','ProductAmt','BpID', 'Deli_Region','OrderTime','ProductCategory','ProductClass','Region',\n",
    "              'ConstructionType','OrderDate','BpType','BpClass']\n",
    "duration = datetime.datetime.now()-start\n",
    "m, s = divmod(duration.seconds, 60);h, m = divmod(m, 60);print(\"[%02d:%02d:%02d]\" %(h, m, s))\n",
    "df = df[['OrderNum','OrderTime','OrderDate','ProductCode','ProductCategory', 'ProductClass','ProductAmt', \n",
    "    'Deli_Region','BpID','Region','ConstructionType','BpType', 'BpClass']]\n",
    "# 이상 데이터 불러오기 및 필요 데이터 필터링 부분.\n",
    "df.to_csv('./Datasets/df_181106_new.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./Datasets/df_181106_new.csv', dtype='str', encoding = 'utf-8')\n",
    "# df = df.drop(['OrderNum', 'OrderTime', 'Deli_Region'],axis = 1)\n",
    "# df['OrderDate'] = pd.to_datetime(df['OrderDate']).dt.date\n",
    "# df1 = df.groupby(['BpID','OrderDate','ProductCode','ProductCategory',\n",
    "#             'ProductClass','Region','ConstructionType','BpType','BpClass'])['ProductAmt'].sum()\n",
    "# df1 = df1.reset_index().sort_values(['BpID','OrderDate']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx =  3 # 이전 기록 갯수(X)\n",
    "# ny = 1 # 이후 기록 갯수(Y)\n",
    "\n",
    "# targetdf=None\n",
    "# Bplist = df1.BpID.unique()\n",
    "# for ii,jj in enumerate(Bplist):\n",
    "#     if ii % 50 ==0:\n",
    "#         print(ii, len(Bplist))\n",
    "#     if len(df1[df1.BpID ==jj].OrderDate.unique())>=nx+ny:\n",
    "#         targetdata = df1[df1.BpID ==jj]\n",
    "#         targetdate = targetdata.OrderDate.unique()\n",
    "#         for kk in range(len(targetdate)-nx):\n",
    "#             # (subset='ProductCode') 제거할지 추후 고려\n",
    "#             x1 = targetdata[targetdata.OrderDate.isin(targetdate[kk:kk+nx])].drop_duplicates(subset='ProductCode')\n",
    "#             y1 = targetdata[targetdata.OrderDate.isin(targetdate[kk+nx:kk+nx+ny])].drop_duplicates(subset='ProductCode')\n",
    "#             x2 = pd.concat([x1]*y1.shape[0]).reset_index(drop=True)\n",
    "#             y2 = pd.concat([y1[['OrderDate','ProductCode']]]*x1.shape[0]).sort_values(by='ProductCode').reset_index(drop=True)\n",
    "#             try:\n",
    "#                 targetdf = pd.concat([targetdf,pd.merge(x2, y2,left_index=True, right_index=True)])\n",
    "#             except NameError:\n",
    "#                 targetdf = pd.merge(x2, y2,left_index=True, right_index=True)\n",
    "#     else:\n",
    "#         continue\n",
    "# # 프레딕트 할때는 최근 3일 결과만 준비하면 된다.\n",
    "# # 조온나 오래걸림.\n",
    "# targetdf.to_csv(\"./Datasets/df_181108_new.csv\",encoding='utf-8') # 미리 tobe파일명 반영."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetdf = pd.read_csv('df_181108_new.csv', dtype='str', encoding = 'utf-8', index_col= 0)\n",
    "targetdf['DateGap'] = (pd.to_datetime(targetdf['OrderDate_y'])-pd.to_datetime(targetdf['OrderDate_x'])).dt.days\n",
    "# targetdf['ProductAmt']=targetdf['ProductAmt'].astype('int64') # ProductAmt 재가공 필요 잇음. groupby에서부터\n",
    "targetdf1 = targetdf.drop(['BpID','OrderDate_x','OrderDate_y','ProductAmt'],axis = 1)\n",
    "targetdf2 = targetdf1[['Region','BpType','BpClass','ConstructionType','ProductCode_x','ProductCategory',\n",
    "                       'ProductClass','DateGap','ProductCode_y']]\n",
    "\n",
    "train_df, eval_df = train_test_split(targetdf2.values, test_size=0.2, shuffle=False)\n",
    "\n",
    "np.savetxt(fname='C:/Users/aj901/Desktop/Wide&Deep_논문/SK_Telesis/new_dataset/train_target_2.csv', X=train_df, delimiter=',',encoding='utf-8',fmt=\"%s\")\n",
    "np.savetxt(fname='C:/Users/aj901/Desktop/Wide&Deep_논문/SK_Telesis/new_dataset/eval_target_2.csv', X=eval_df, delimiter=',',encoding='utf-8',fmt=\"%s\")\n",
    "\n",
    "model_dir = \"./Trained_models/Models_WnD/folder_test\"\n",
    "n_classes = len(targetdf2.ProductCode_y.unique())\n",
    "voca_classes = tuple(targetdf2.ProductCode_y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(data_files, num_epochs, batch_size, predict=False):\n",
    "    def parse_csv(value):\n",
    "        columns = tf.decode_csv(value, record_defaults=[[\"\"]]*7+[[0]]*1+[[\"\"]])\n",
    "        features = dict(zip(targetdf2.columns[:8], columns[:8]))\n",
    "        label = columns[-1]\n",
    "        out = features, label\n",
    "        return out\n",
    "    \n",
    "    dataset = tf.data.TextLineDataset(data_files)\n",
    "    if predict==False:\n",
    "        dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(buffer_size = 100, count = num_epochs))\n",
    "    dataset = dataset.apply(tf.contrib.data.map_and_batch(map_func=parse_csv, \n",
    "                                                          batch_size=batch_size, \n",
    "                                                          num_parallel_calls = 16))\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(buffer_size = 50)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels\n",
    "\n",
    "train_filenames = [\"C:/Users/aj901/Desktop/Wide&Deep_논문/SK_Telesis/new_dataset/train_target.csv\"]\n",
    "test_filenames = [\"C:/Users/aj901/Desktop/Wide&Deep_논문/SK_Telesis/new_dataset/test_target.csv\"]\n",
    "\n",
    "def train_input_fn():\n",
    "    return input_fn(data_files=train_filenames,num_epochs=int(1e6), batch_size=64)\n",
    "\n",
    "def eval_input_fn():\n",
    "    return input_fn(data_files=test_filenames, num_epochs=1, batch_size=1)\n",
    "\n",
    "def pred_input_fn():\n",
    "    return input_fn(data_files=test_filenames,num_epochs=1,batch_size=1,predict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n",
      "C:\\Users\\aj901\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\tf_logging.py:120: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  _get_logger().warn(msg, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "Region = tf.contrib.layers.sparse_column_with_hash_bucket(\"Region\", hash_bucket_size=300)\n",
    "BpType=tf.contrib.layers.sparse_column_with_hash_bucket(\"BpType\", hash_bucket_size=300)\n",
    "BpClass=tf.contrib.layers.sparse_column_with_hash_bucket(\"BpClass\", hash_bucket_size=300)\n",
    "ConstructionType=tf.contrib.layers.sparse_column_with_hash_bucket(\"ConstructionType\", hash_bucket_size=100)\n",
    "ProductCategory=tf.contrib.layers.sparse_column_with_hash_bucket(\"ProductCategory\", hash_bucket_size=300)\n",
    "ProductClass=tf.contrib.layers.sparse_column_with_hash_bucket(\"ProductClass\", hash_bucket_size=300)\n",
    "# ProductAmt=tf.contrib.layers.real_valued_column(\"ProductAmt\", dtype=tf.int64)\n",
    "\n",
    "DateGap = tf.contrib.layers.real_valued_column(\"DateGap\", dtype=tf.int64)\n",
    "DateGap_buckets = tf.contrib.layers.bucketized_column(\n",
    "    source_column=DateGap,\n",
    "    boundaries=[5*(ii+1) for ii in range(73)])\n",
    "\n",
    "ProductCode_x = tf.contrib.layers.sparse_column_with_hash_bucket(\"ProductCode_x\", hash_bucket_size=20000)\n",
    "\n",
    "wide_columns = [\n",
    "    ProductCode_x, \n",
    "    DateGap_buckets,\n",
    "    tf.contrib.layers.crossed_column([ProductCode_x, ConstructionType], hash_bucket_size=530000)\n",
    "]\n",
    "deep_columns = [\n",
    "    DateGap, \n",
    "#     ProductAmt,\n",
    "    tf.contrib.layers.embedding_column(BpClass, dimension=8), \n",
    "    tf.contrib.layers.embedding_column(BpType, dimension=8), \n",
    "    tf.contrib.layers.embedding_column(ConstructionType, dimension=8), \n",
    "    tf.contrib.layers.embedding_column(Region, dimension=8),\n",
    "    tf.contrib.layers.embedding_column(ProductCategory, dimension=8),\n",
    "    tf.contrib.layers.embedding_column(ProductClass, dimension=8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': './Trained_models/Models_WnD/folder_test', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 128\n",
      ", '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002A2B1AC1860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# model config setup for speed up (reference : tensorflow performance guide)\n",
    "config = tf.ConfigProto()\n",
    "config.intra_op_parallelism_threads = 128 # 8->16->32->64....\n",
    "config.inter_op_parallelism_threads = 0 # 0으로 고정\n",
    "_config = tf.estimator.RunConfig(\n",
    "    session_config=config, \n",
    "    save_checkpoints_secs=None, \n",
    "    save_checkpoints_steps=1000,\n",
    "    keep_checkpoint_max=1)\n",
    "\n",
    "tot_model = tf.estimator.DNNLinearCombinedClassifier(\n",
    "    model_dir=model_dir, \n",
    "    linear_feature_columns=wide_columns, \n",
    "    dnn_feature_columns=deep_columns, \n",
    "    dnn_hidden_units= [256,128], \n",
    "    dnn_dropout = 0.7, \n",
    "    dnn_optimizer= tf.train.AdamOptimizer,\n",
    "    n_classes = n_classes,\n",
    "    label_vocabulary= voca_classes,\n",
    "    batch_norm = True,\n",
    "    config = _config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1000 or save_checkpoints_secs None.\n",
      "WARNING:tensorflow:From <ipython-input-5-7cae420c6cd0>:11: shuffle_and_repeat (from tensorflow.contrib.data.python.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.shuffle_and_repeat(...)`.\n",
      "WARNING:tensorflow:From <ipython-input-5-7cae420c6cd0>:14: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.map_and_batch(...)`.\n",
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\feature_column.py:2391: calling sparse_feature_cross (from tensorflow.contrib.layers.python.ops.sparse_feature_cross_op) with hash_key=None is deprecated and will be removed after 2016-11-20.\n",
      "Instructions for updating:\n",
      "The default behavior of sparse_feature_cross is changing, the default\n",
      "value for hash_key will change to SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY.\n",
      "From that point on sparse_feature_cross will always use FingerprintCat64\n",
      "to concatenate the feature fingerprints. And the underlying\n",
      "_sparse_feature_cross_op.sparse_feature_cross operation will be marked\n",
      "as deprecated.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 530000\n  }\n  dim {\n    size: 13936\n  }\n}\nfloat_val: 0.1\n\n\t [[node head/Const (defined at <ipython-input-9-284426665103>:4)  = Const[_class=[\"loc:@linea...trl/Assign\"], dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 530000 } dim { size: 13936 } } float_val: 0.1>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'head/Const', defined at:\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-284426665103>\", line 4, in <module>\n    tf.estimator.train_and_evaluate(tot_model, train_spec, eval_spec)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 471, in train_and_evaluate\n    return executor.run()\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 610, in run\n    return self.run_local()\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 711, in run_local\n    saving_listeners=saving_listeners)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 354, in train\n    loss = self._train_model(input_fn, hooks, saving_listeners)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 1207, in _train_model\n    return self._train_model_default(input_fn, hooks, saving_listeners)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 1237, in _train_model_default\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 1195, in _call_model_fn\n    model_fn_results = self._model_fn(features=features, **kwargs)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\dnn_linear_combined.py\", line 439, in _model_fn\n    linear_sparse_combiner=linear_sparse_combiner)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\dnn_linear_combined.py\", line 238, in _dnn_linear_combined_model_fn\n    logits=logits)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\head.py\", line 239, in create_estimator_spec\n    regularization_losses))\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\head.py\", line 873, in _create_tpu_estimator_spec\n    train_op = train_op_fn(regularized_training_loss)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\dnn_linear_combined.py\", line 227, in _train_op_fn\n    scope=linear_absolute_scope)))\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 410, in minimize\n    name=name)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 593, in apply_gradients\n    self._create_slots(var_list)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\ftrl.py\", line 127, in _create_slots\n    self._initial_accumulator_value, dtype=v.dtype, shape=v.get_shape())\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 214, in constant\n    name=name).outputs[0]\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 530000\n  }\n  dim {\n    size: 13936\n  }\n}\nfloat_val: 0.1\n\n\t [[node head/Const (defined at <ipython-input-9-284426665103>:4)  = Const[_class=[\"loc:@linea...trl/Assign\"], dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 530000 } dim { size: 13936 } } float_val: 0.1>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 530000\n  }\n  dim {\n    size: 13936\n  }\n}\nfloat_val: 0.1\n\n\t [[{{node head/Const}} = Const[_class=[\"loc:@linea...trl/Assign\"], dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 530000 } dim { size: 13936 } } float_val: 0.1>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-284426665103>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrain_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrainSpec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0meval_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEvalSpec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_and_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtot_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(estimator, train_spec, eval_spec)\u001b[0m\n\u001b[0;32m    469\u001b[0m         '(with task id 0).  Given task id {}'.format(config.task_id))\n\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    608\u001b[0m         config.task_type != run_config_lib.TaskType.EVALUATOR):\n\u001b[0;32m    609\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Running training and evaluation locally (non-distributed).'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m     \u001b[1;31m# Distributed case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\u001b[0m in \u001b[0;36mrun_local\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[0mmax_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_hooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m         saving_listeners=saving_listeners)\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m     eval_result = listener_for_eval.eval_result or _EvalResult(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m       \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1205\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1207\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1209\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1239\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[0;32m   1240\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1241\u001b[1;33m                                              saving_listeners)\n\u001b[0m\u001b[0;32m   1242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[1;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[0;32m   1466\u001b[0m         \u001b[0msave_summaries_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_summary_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1467\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_config\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1468\u001b[1;33m         log_step_count_steps=log_step_count_steps) as mon_sess:\n\u001b[0m\u001b[0;32m   1469\u001b[0m       \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1470\u001b[0m       \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mMonitoredTrainingSession\u001b[1;34m(master, is_chief, checkpoint_dir, scaffold, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps, max_wait_secs, save_checkpoint_steps, summary_dir)\u001b[0m\n\u001b[0;32m    502\u001b[0m       \u001b[0msession_creator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession_creator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_hooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m       stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, session_creator, hooks, stop_grace_period_secs)\u001b[0m\n\u001b[0;32m    919\u001b[0m     super(MonitoredSession, self).__init__(\n\u001b[0;32m    920\u001b[0m         \u001b[0msession_creator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, session_creator, hooks, should_recover, stop_grace_period_secs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0;32m    642\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_RecoverableSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    644\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sess_creator)\u001b[0m\n\u001b[0;32m   1105\u001b[0m     \"\"\"\n\u001b[0;32m   1106\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess_creator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess_creator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1107\u001b[1;33m     \u001b[0m_WrappedSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36m_create_session\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1110\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1112\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess_creator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1113\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m         logging.info('An error was raised while a session was being created. '\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mcreate_session\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    798\u001b[0m       \u001b[1;34m\"\"\"Creates a coordinated session.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m       \u001b[1;31m# Keep the tf_sess for unit testing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_sess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_creator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    801\u001b[0m       \u001b[1;31m# We don't want coordinator to suppress any exception.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoord\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoordinator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCoordinator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_stop_exception_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mcreate_session\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    564\u001b[0m         \u001b[0minit_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[0minit_feed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_feed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m         init_fn=self._scaffold.init_fn)\n\u001b[0m\u001b[0;32m    567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\session_manager.py\u001b[0m in \u001b[0;36mprepare_session\u001b[1;34m(self, master, init_op, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config, init_feed_dict, init_fn)\u001b[0m\n\u001b[0;32m    292\u001b[0m                            \"init_fn or local_init_op was given\")\n\u001b[0;32m    293\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0minit_op\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_feed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[0minit_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 530000\n  }\n  dim {\n    size: 13936\n  }\n}\nfloat_val: 0.1\n\n\t [[node head/Const (defined at <ipython-input-9-284426665103>:4)  = Const[_class=[\"loc:@linea...trl/Assign\"], dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 530000 } dim { size: 13936 } } float_val: 0.1>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'head/Const', defined at:\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-284426665103>\", line 4, in <module>\n    tf.estimator.train_and_evaluate(tot_model, train_spec, eval_spec)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 471, in train_and_evaluate\n    return executor.run()\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 610, in run\n    return self.run_local()\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 711, in run_local\n    saving_listeners=saving_listeners)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 354, in train\n    loss = self._train_model(input_fn, hooks, saving_listeners)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 1207, in _train_model\n    return self._train_model_default(input_fn, hooks, saving_listeners)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 1237, in _train_model_default\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 1195, in _call_model_fn\n    model_fn_results = self._model_fn(features=features, **kwargs)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\dnn_linear_combined.py\", line 439, in _model_fn\n    linear_sparse_combiner=linear_sparse_combiner)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\dnn_linear_combined.py\", line 238, in _dnn_linear_combined_model_fn\n    logits=logits)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\head.py\", line 239, in create_estimator_spec\n    regularization_losses))\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\head.py\", line 873, in _create_tpu_estimator_spec\n    train_op = train_op_fn(regularized_training_loss)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\dnn_linear_combined.py\", line 227, in _train_op_fn\n    scope=linear_absolute_scope)))\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 410, in minimize\n    name=name)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 593, in apply_gradients\n    self._create_slots(var_list)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\ftrl.py\", line 127, in _create_slots\n    self._initial_accumulator_value, dtype=v.dtype, shape=v.get_shape())\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 214, in constant\n    name=name).outputs[0]\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 530000\n  }\n  dim {\n    size: 13936\n  }\n}\nfloat_val: 0.1\n\n\t [[node head/Const (defined at <ipython-input-9-284426665103>:4)  = Const[_class=[\"loc:@linea...trl/Assign\"], dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 530000 } dim { size: 13936 } } float_val: 0.1>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# # https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)\n",
    "tf.estimator.train_and_evaluate(tot_model, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Could not find trained model in model_dir: ./Trained_models/Models_WnD/folder_test, running initialization to evaluate.\n",
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aj901\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-02-13-01:24:41\n",
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    }
   ],
   "source": [
    "tot_model.evaluate(input_fn=eval_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # 커스텀 평가기준\n",
    "# custom_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/performance/datasets_performance\n",
    "#https://www.tensorflow.org/performance/performance_guide\n",
    "#https://www.tensorflow.org/performance/performance_models\n",
    "\"\"\"\n",
    "181107 테스트로 돌려본 모델\n",
    "◆ 구조를 달리해서 데이터 임포트부터 작업시작.\n",
    "◆ Row를 아이템 베이스로 작업한 버전임.(3일/1일)\n",
    "◎ 결과\n",
    "◆ 이후 아래 데이터 임포트로 작업한 Df파일을 기반으로 아이템의 지정여부, 카테고리등의 데이터를 추가하여 실험.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
